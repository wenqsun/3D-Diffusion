# Repository for 3D Diffusion Generation Research
This is a repository for 3D Diffusion Generation research papers digest. The taxonomy and papers highly refer the paper (state of the art on diffusion models for visual computing) and the repo: https://github.com/cwchenwang/awesome-3d-diffusion?tab=readme-ov-file

## Survey Paper
1. [state of the art on diffusion models for visual computing](https://arxiv.org/abs/2310.07204) ðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸ \
   This survey paper provides insightful introductions about diffusion models and applications, including 2D, 3D, video, and 4D.

## Direct 3D Generation via Diffusion Models
This series of papers are focused on modeling the distribution of 3D shapes. Eventually, 3D content/model can be directly generated by the well-trained 3D diffusion models.
<img width="1310" alt="image" src="https://github.com/wenqsun/3D-Diffusion/assets/93043187/21e134a2-a110-40aa-9638-1cd9bc11e8bc">
Taxonomy: the type of output, representation,  

## Multiview 2D-to-3D Generation via Diffusion Models
This series of papers aim to leverage the 2D diffusion models (pre-trained or train from scratch) to generate high-quality and diverse 

### Text-to-3D using Pre-trained Image Diffusion Models
1. [DreamFusion: Text-to-3D using 2D Diffusion](https://arxiv.org/abs/2209.14988) ðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸ $\textit{ICLR 2023 Outstanding Paper Award}$\
   This paper firstly proposes using SDS to generate 3D contents based on the pre-trained diffusion models.
2. [Magic3D: High-Resolution Text-to-3D Content Creation](https://research.nvidia.com/labs/dir/magic3d/) ðŸŒŸðŸŒŸðŸŒŸðŸŒŸ $\textit{CVPR 2023}$\
   Based on the SDS, they propose 

### Adapting Image Models for Multi-view Synthesis

### 3D-Aware Image Diffusion

