# Repository for 3D Diffusion Generation Papers
This is a repository for 3D Diffusion Generation research papers digest. The taxonomy and papers highly refer the survey paper (state of the art on diffusion models for visual computing) and the repo: https://github.com/cwchenwang/awesome-3d-diffusion?tab=readme-ov-file

## Survey Paper
1. [state of the art on diffusion models for visual computing](https://arxiv.org/abs/2310.07204) ðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸ \
   This survey paper provides insightful introductions about diffusion models and critical applications, including 2D, 3D, video, and 4D.

## Direct 3D Generation via Diffusion Models
This series of papers are focused on modeling the distribution of 3D shapes. Eventually, 3D content/model can be directly generated by the well-trained 3D diffusion models.
<img width="1310" alt="image" src="https://github.com/wenqsun/3D-Diffusion/assets/93043187/21e134a2-a110-40aa-9638-1cd9bc11e8bc">
Taxonomy: the type of output, representation,  

## Multiview 2D-to-3D Generation via Diffusion Models
This series of papers aim to leverage the 2D diffusion models (pre-trained or train from scratch) to generate high-quality and diverse 3D content. In this mode, the 3D contents are not generated directly by diffusion models.

### Text-to-3D using Pre-trained Image Diffusion Models (Text-to-3D)
1. [DreamFusion: Text-to-3D using 2D Diffusion](https://arxiv.org/abs/2209.14988) ðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸ $\textit{ICLR 2023 Outstanding Paper Award}$\
   This paper firstly proposes using SDS to generate 3D contents based on the pre-trained diffusion models. (NeRF)
2. [Magic3D: High-Resolution Text-to-3D Content Creation](https://research.nvidia.com/labs/dir/magic3d/) ðŸŒŸðŸŒŸðŸŒŸðŸŒŸ $\textit{CVPR 2023}$\
   Based on the SDS, they propose the coarse-to-fine two-stage optimization to generate high-resolution 3D output efficiently. (NeRF & Mesh)
3. [Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation](https://arxiv.org/abs/2303.13873) ðŸŒŸðŸŒŸðŸŒŸ $\textit{ICCV 2023}$\
   They
5. [ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation](https://arxiv.org/abs/2305.16213) ðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸ $\textit{NeurIPS 2023 Spotlight}$\
   This paper proposes the VSD to enhance the quality and diversity of 3D contents. (NeRF & Mesh)
6. [NFSD: Noise Free Score Distillation](https://arxiv.org/abs/2310.17590) 

### Adapting Image Models for Multi-view Synthesis (Image-to-3D)
1. [RealFusion: 360Â° Reconstruction of Any Object from a Single Image](https://arxiv.org/abs/2302.10663) ðŸŒŸðŸŒŸðŸŒŸ $\textit{CVPR 2023}$\
   
2. [Zero-1-to-3: Zero-shot One Image to 3D Object](https://arxiv.org/abs/2303.11328) ðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸ $\textit{ICCV 2023}$\
   Zero-shot transfer, single image input, and 3D generation content (meanings of zero, one, and three).
3. [Magic123: One Image to High-Quality 3D Object Generation Using Both 2D and 3D Diffusion Priors](https://arxiv.org/abs/2306.17843) ðŸŒŸðŸŒŸðŸŒŸðŸŒŸ $\textit{ICCV 2023}$\
   The idea is similar to Magic which is composed of two stages. Single image input, 2D prior, and 3D prior (meanings of one, two, three).
4. 

### 3D-Aware Image Diffusion

